{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-in-csv/mnist_test.csv\n/kaggle/input/mnist-in-csv/mnist_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Opening the test and train data to save into variables\ntrain_path = '/kaggle/input/mnist-in-csv/mnist_train.csv'\ntest_path = '/kaggle/input/mnist-in-csv/mnist_test.csv'\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\n\n#Here I am defining a function that will let us visualize the data and the accompanying label\ndef rand_example(data):\n    #Selecting a random data point from our data\n    rand_num = random.randrange(0,data.shape[0])\n    rand_data = data.iloc[rand_num]\n    \n    #splitting the labels and pixel values\n    label = rand_data[0]\n    pixels = np.array(rand_data[1:])\n    print(pixels.shape)\n    \n    #reshaping pixel values to be in the correct shape\n    pixels = pixels.reshape((28,28))\n    print(pixels)\n    \n    plt.title(f'Label is {label}')\n    plt.imshow(pixels, cmap='gray')\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"rand_example(test_data)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(784,)\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0  12  47  69 150 210 255 254 254\n   45   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0  48  94 142 211 253 253 253 200 160 210 253\n  129   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0  52 140 174 249 253 251 217 215 114  31   4   0  50 253\n  129   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0  63 250 253 177  99  63   0   0   0   0   0   0 135 253\n   95   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0  20  20   2   0   0   0   0   0   0   0   0 188 253\n   25   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 239 253\n   25   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  42 246 217\n   14   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  89 253 150\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   6 134 253  76\n    3   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0  44 117 156 252 253 253 253\n  205 156  44   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0 136 202 245 253 253 253 253 241 155\n  244 253 179   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0 206 218 109  87   5 147 253  61   0\n    4 144 119   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0 120 191   0   0   0 224 186   4   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   7  54   0   0 107 253 165   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 202 253  91   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0  26 228 224  32   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0  80 253 109   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   1 160 253  62   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   6 253 253   5   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   6 253 213   3   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(60000, 785)"},"metadata":{}}]},{"cell_type":"code","source":"def data_transform(data):\n    length = data.shape[0]\n    labels = data['label']\n    pixels = data.drop('label', axis=1)\n\n    labels = np.array(labels)\n    pixels = np.array(pixels)\n\n\n    pixels = pixels.reshape((length,28,28))\n    return pixels,labels\n\nX_train, y_train = data_transform(train_data)\nX_test, y_test = data_transform(test_data)\n\nprint(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)\n","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"(60000, 28, 28) (60000,)\n(10000, 28, 28) (10000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nX_train = torch.from_numpy(X_train)\ny_train = torch.from_numpy(y_train)\n\nX_train = X_train.type(torch.FloatTensor)\ny_train = y_train.type(torch.LongTensor)\n\nX_test = torch.from_numpy(X_test)\ny_test = torch.from_numpy(y_test)\n\nX_test = X_test.type(torch.FloatTensor)\ny_test = y_test.type(torch.LongTensor)\n","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(X_test.dtype)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"torch.float32\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_set = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = TensorDataset(X_test, y_test)\ntest_set = DataLoader(test_dataset, batch_size=64,shuffle = True)\n\nfor data in train_set:\n    print(data)\n    break","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([3, 2, 5, 8, 4, 0, 2, 2, 0, 2, 4, 9, 8, 7, 2, 1, 5, 0, 7, 5, 8, 1, 1, 4,\n        1, 8, 9, 3, 6, 1, 0, 8, 2, 3, 1, 7, 0, 7, 7, 1, 2, 9, 3, 4, 2, 0, 9, 2,\n        1, 8, 4, 1, 6, 0, 3, 5, 3, 8, 2, 0, 7, 3, 0, 9])]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model = NeuralNetwork()\nprint(model)","metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=512, out_features=10, bias=True)\n    (9): ReLU()\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= size\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-4\n#batch_size = 64\nepochs = 20\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for t in range(epochs):\n    print(f\"Epoch {t+1} - Learning Rate: {learning_rate}\\n-------------------------------\")\n    train_loop(train_set, model, loss_fn, optimizer)\n    test_loop(test_set, model, loss_fn)\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1 - Learning Rate: 0.0001\n-------------------------------\nloss: 2.732297  [    0/60000]\nloss: 2.125039  [ 6400/60000]\nloss: 1.984109  [12800/60000]\nloss: 1.894516  [19200/60000]\nloss: 1.880681  [25600/60000]\nloss: 1.870952  [32000/60000]\nloss: 1.595463  [38400/60000]\nloss: 1.666177  [44800/60000]\nloss: 1.616250  [51200/60000]\nloss: 1.763234  [57600/60000]\nTest Error: \n Accuracy: 51.6%, Avg loss: 0.023842 \n\nEpoch 2 - Learning Rate: 0.0001\n-------------------------------\nloss: 1.328708  [    0/60000]\nloss: 1.413822  [ 6400/60000]\nloss: 1.333464  [12800/60000]\nloss: 1.558302  [19200/60000]\nloss: 1.370843  [25600/60000]\nloss: 1.329203  [32000/60000]\nloss: 1.196300  [38400/60000]\nloss: 1.126177  [44800/60000]\nloss: 1.176456  [51200/60000]\nloss: 1.089009  [57600/60000]\nTest Error: \n Accuracy: 62.3%, Avg loss: 0.019580 \n\nEpoch 3 - Learning Rate: 0.0001\n-------------------------------\nloss: 1.508613  [    0/60000]\nloss: 1.318599  [ 6400/60000]\nloss: 1.252593  [12800/60000]\nloss: 1.004428  [19200/60000]\nloss: 1.045494  [25600/60000]\nloss: 1.088336  [32000/60000]\nloss: 1.077483  [38400/60000]\nloss: 0.935168  [44800/60000]\nloss: 1.042208  [51200/60000]\nloss: 0.740710  [57600/60000]\nTest Error: \n Accuracy: 71.9%, Avg loss: 0.014611 \n\nEpoch 4 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.859853  [    0/60000]\nloss: 0.630833  [ 6400/60000]\nloss: 0.986659  [12800/60000]\nloss: 0.807949  [19200/60000]\nloss: 1.125270  [25600/60000]\nloss: 0.927487  [32000/60000]\nloss: 0.698748  [38400/60000]\nloss: 0.498985  [44800/60000]\nloss: 0.561146  [51200/60000]\nloss: 0.514844  [57600/60000]\nTest Error: \n Accuracy: 81.2%, Avg loss: 0.009779 \n\nEpoch 5 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.543079  [    0/60000]\nloss: 0.611100  [ 6400/60000]\nloss: 0.564692  [12800/60000]\nloss: 0.581888  [19200/60000]\nloss: 0.531286  [25600/60000]\nloss: 0.616556  [32000/60000]\nloss: 0.600839  [38400/60000]\nloss: 0.577600  [44800/60000]\nloss: 0.550557  [51200/60000]\nloss: 0.628968  [57600/60000]\nTest Error: \n Accuracy: 82.2%, Avg loss: 0.008659 \n\nEpoch 6 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.657925  [    0/60000]\nloss: 0.385261  [ 6400/60000]\nloss: 0.486960  [12800/60000]\nloss: 0.505389  [19200/60000]\nloss: 0.414777  [25600/60000]\nloss: 0.623628  [32000/60000]\nloss: 0.377066  [38400/60000]\nloss: 0.628978  [44800/60000]\nloss: 0.489329  [51200/60000]\nloss: 0.576179  [57600/60000]\nTest Error: \n Accuracy: 82.9%, Avg loss: 0.008112 \n\nEpoch 7 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.354081  [    0/60000]\nloss: 0.424124  [ 6400/60000]\nloss: 0.426066  [12800/60000]\nloss: 0.326678  [19200/60000]\nloss: 0.548492  [25600/60000]\nloss: 0.747153  [32000/60000]\nloss: 0.549229  [38400/60000]\nloss: 0.396472  [44800/60000]\nloss: 0.498967  [51200/60000]\nloss: 0.551915  [57600/60000]\nTest Error: \n Accuracy: 83.4%, Avg loss: 0.007711 \n\nEpoch 8 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.459493  [    0/60000]\nloss: 0.490560  [ 6400/60000]\nloss: 0.490704  [12800/60000]\nloss: 0.250104  [19200/60000]\nloss: 0.556376  [25600/60000]\nloss: 0.496780  [32000/60000]\nloss: 0.487107  [38400/60000]\nloss: 0.293853  [44800/60000]\nloss: 0.628963  [51200/60000]\nloss: 0.379529  [57600/60000]\nTest Error: \n Accuracy: 83.6%, Avg loss: 0.007485 \n\nEpoch 9 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.469055  [    0/60000]\nloss: 0.544339  [ 6400/60000]\nloss: 0.740878  [12800/60000]\nloss: 0.581303  [19200/60000]\nloss: 0.455782  [25600/60000]\nloss: 0.306107  [32000/60000]\nloss: 0.583382  [38400/60000]\nloss: 0.567184  [44800/60000]\nloss: 0.577936  [51200/60000]\nloss: 0.382891  [57600/60000]\nTest Error: \n Accuracy: 84.1%, Avg loss: 0.007284 \n\nEpoch 10 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.309056  [    0/60000]\nloss: 0.594400  [ 6400/60000]\nloss: 0.289748  [12800/60000]\nloss: 0.396446  [19200/60000]\nloss: 0.265647  [25600/60000]\nloss: 0.382932  [32000/60000]\nloss: 0.286056  [38400/60000]\nloss: 0.511722  [44800/60000]\nloss: 0.640347  [51200/60000]\nloss: 0.620037  [57600/60000]\nTest Error: \n Accuracy: 84.4%, Avg loss: 0.007060 \n\nEpoch 11 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.502887  [    0/60000]\nloss: 0.402058  [ 6400/60000]\nloss: 0.583710  [12800/60000]\nloss: 0.383938  [19200/60000]\nloss: 0.417731  [25600/60000]\nloss: 0.517428  [32000/60000]\nloss: 0.399106  [38400/60000]\nloss: 0.509444  [44800/60000]\nloss: 0.588190  [51200/60000]\nloss: 0.379031  [57600/60000]\nTest Error: \n Accuracy: 84.6%, Avg loss: 0.006984 \n\nEpoch 12 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.635111  [    0/60000]\nloss: 0.512429  [ 6400/60000]\nloss: 0.442184  [12800/60000]\nloss: 0.467976  [19200/60000]\nloss: 0.290695  [25600/60000]\nloss: 0.525420  [32000/60000]\nloss: 0.384334  [38400/60000]\nloss: 0.346245  [44800/60000]\nloss: 0.441123  [51200/60000]\nloss: 0.448022  [57600/60000]\nTest Error: \n Accuracy: 84.9%, Avg loss: 0.006811 \n\nEpoch 13 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.430626  [    0/60000]\nloss: 0.283267  [ 6400/60000]\nloss: 0.428990  [12800/60000]\nloss: 0.680935  [19200/60000]\nloss: 0.597607  [25600/60000]\nloss: 0.313704  [32000/60000]\nloss: 0.357687  [38400/60000]\nloss: 0.638074  [44800/60000]\nloss: 0.422350  [51200/60000]\nloss: 0.410869  [57600/60000]\nTest Error: \n Accuracy: 85.0%, Avg loss: 0.006695 \n\nEpoch 14 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.679226  [    0/60000]\nloss: 0.230873  [ 6400/60000]\nloss: 0.543355  [12800/60000]\nloss: 0.305203  [19200/60000]\nloss: 0.357350  [25600/60000]\nloss: 0.361099  [32000/60000]\nloss: 0.439380  [38400/60000]\nloss: 0.494902  [44800/60000]\nloss: 0.272781  [51200/60000]\nloss: 0.351124  [57600/60000]\nTest Error: \n Accuracy: 85.1%, Avg loss: 0.006645 \n\nEpoch 15 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.499919  [    0/60000]\nloss: 0.363287  [ 6400/60000]\nloss: 0.514149  [12800/60000]\nloss: 0.344519  [19200/60000]\nloss: 0.451650  [25600/60000]\nloss: 0.299507  [32000/60000]\nloss: 0.395921  [38400/60000]\nloss: 0.385592  [44800/60000]\nloss: 0.251468  [51200/60000]\nloss: 0.365927  [57600/60000]\nTest Error: \n Accuracy: 85.5%, Avg loss: 0.006512 \n\nEpoch 16 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.630002  [    0/60000]\nloss: 0.387802  [ 6400/60000]\nloss: 0.399626  [12800/60000]\nloss: 0.414083  [19200/60000]\nloss: 0.579297  [25600/60000]\nloss: 0.423747  [32000/60000]\nloss: 0.413549  [38400/60000]\nloss: 0.472233  [44800/60000]\nloss: 0.346045  [51200/60000]\nloss: 0.404893  [57600/60000]\nTest Error: \n Accuracy: 85.5%, Avg loss: 0.006413 \n\nEpoch 17 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.324620  [    0/60000]\nloss: 0.686746  [ 6400/60000]\nloss: 0.347051  [12800/60000]\nloss: 0.455993  [19200/60000]\nloss: 0.341563  [25600/60000]\nloss: 0.337730  [32000/60000]\nloss: 0.557176  [38400/60000]\nloss: 0.404358  [44800/60000]\nloss: 0.380942  [51200/60000]\nloss: 0.288498  [57600/60000]\nTest Error: \n Accuracy: 85.5%, Avg loss: 0.006357 \n\nEpoch 18 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.464402  [    0/60000]\nloss: 0.459957  [ 6400/60000]\nloss: 0.342031  [12800/60000]\nloss: 0.383038  [19200/60000]\nloss: 0.389420  [25600/60000]\nloss: 0.438966  [32000/60000]\nloss: 0.462423  [38400/60000]\nloss: 0.185002  [44800/60000]\nloss: 0.890320  [51200/60000]\nloss: 0.463324  [57600/60000]\nTest Error: \n Accuracy: 85.7%, Avg loss: 0.006292 \n\nEpoch 19 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.328840  [    0/60000]\nloss: 0.389101  [ 6400/60000]\nloss: 0.353625  [12800/60000]\nloss: 0.170221  [19200/60000]\nloss: 0.293465  [25600/60000]\nloss: 0.620010  [32000/60000]\nloss: 0.519039  [38400/60000]\nloss: 0.510892  [44800/60000]\nloss: 0.343207  [51200/60000]\nloss: 0.493392  [57600/60000]\nTest Error: \n Accuracy: 85.7%, Avg loss: 0.006247 \n\nEpoch 20 - Learning Rate: 0.0001\n-------------------------------\nloss: 0.300808  [    0/60000]\nloss: 0.401465  [ 6400/60000]\nloss: 0.509765  [12800/60000]\nloss: 0.656887  [19200/60000]\nloss: 0.516785  [25600/60000]\nloss: 0.440394  [32000/60000]\nloss: 0.579262  [38400/60000]\nloss: 0.457687  [44800/60000]\nloss: 0.371115  [51200/60000]\nloss: 0.282846  [57600/60000]\nTest Error: \n Accuracy: 85.8%, Avg loss: 0.006179 \n\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}